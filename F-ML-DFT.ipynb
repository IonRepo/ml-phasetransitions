{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad63bbe-3110-4abf-888e-d93310e0f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from pymatgen.core.structure         import Structure\n",
    "from pymatgen.symmetry.bandstructure import HighSymmKpath\n",
    "\n",
    "from libraries.model   import Helmholtz_free_energy_function, make_predictions, GCNN, compute_coefficients, compute_Fv\n",
    "from libraries.dataset import load_atomic_masses, include_temperatures, create_predictions_dataset, standardize_dataset_from_keys\n",
    "\n",
    "sys.path.append('../../UPC')\n",
    "import MP.MP_library       as MPL\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefb478d-ca97-471e-9605-502e5a53a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder    = 'model'  # Pre-trained model and dataset parameters\n",
    "output_folder   = 'output'  # Output files and figures\n",
    "input_folder    = 'input'  # General files (e.g., atomic masses information)\n",
    "\n",
    "# Whether to plot the harmonic extrapolations of Fv (very time-consuming) or not\n",
    "plot_extrapolations = False\n",
    "\n",
    "# Defining the range of temperatures\n",
    "Ti = 300\n",
    "Tf = 600\n",
    "dT = 5\n",
    "temperatures = np.arange(Ti, Tf+dT, dT)  # Temperatures for prediction of free-energies\n",
    "\n",
    "# Loading dictionary of atomic masses\n",
    "atomic_masses = load_atomic_masses(f'{input_folder}/atomic_masses.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2863981a-8bce-4549-9bd4-62895912297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the JSON file\n",
    "with open(f'{model_folder}/standardized_parameters.json', 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert NumPy arrays back to PyTorch tensors\n",
    "standardized_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        standardized_parameters[key] = torch.tensor(value)\n",
    "    except:\n",
    "        standardized_parameters[key] = value\n",
    "\n",
    "# Load reference dataset for uncertainty estimation\n",
    "reference_dataset = torch.load(f'{model_folder}/ref_std_dataset.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45a3fe7-d3f2-465d-a2ee-460aea60e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b', 'r', 'g', 'k']\n",
    "\n",
    "materials = {\n",
    "    'CO': ['P2_12_12_1', 'R3c'],\n",
    "    'MgTe': ['F-43m', 'P6_3mc'],\n",
    "    'SeN': ['C2-c', 'P2_1-c'],\n",
    "    'SrC2': ['C2-c', 'I4-mmm'],\n",
    "    'WCl6': ['P-3m1', 'R-3'],\n",
    "    'WN2': ['P3_121', 'Pna2_1'],\n",
    "    'WSe2': ['P6_3-mmc', 'P-6m2'],\n",
    "    'ZnCl2': ['P2_1-c', 'P4_2-nmc'],\n",
    "    'ZnTe': ['F-43m', 'P6_3mc'],\n",
    "    'ZrSeO': ['P2_13', 'P4-nmm']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd527101-561c-4081-a9c1-08d140648236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_ML_Fv(path_to_POSCAR, temperatures, standardized_parameters, model_folder, device):\n",
    "    # Create dataset for predictions\n",
    "    dataset = create_predictions_dataset(path_to_POSCAR, path_to_material=True, path_to_polymorph=True)\n",
    "    \n",
    "    labels = [graph.label for graph in dataset]\n",
    "\n",
    "    # Standardize properties\n",
    "    std_dataset = standardize_dataset_from_keys(dataset, standardized_parameters)\n",
    "\n",
    "    # Load Graph Neural Network model (making room for temperature as node attribute) to device\n",
    "    # Dropout for initializing the model, not used at all while predicting\n",
    "    model = GCNN(features_channels=dataset[0].num_node_features+1,\n",
    "                 pdropout=0).to(device)\n",
    "\n",
    "    # Free-up CUDA\n",
    "    del dataset\n",
    "\n",
    "    # Load and evaluate Graph Neural Network model\n",
    "    model.load_state_dict(torch.load(f'{model_folder}/model.pt', map_location=torch.device(device)))\n",
    "    model.eval()\n",
    "\n",
    "    # Include temperatures\n",
    "    std_dataset_w_temp = include_temperatures(std_dataset, temperatures, standardized_parameters)\n",
    "\n",
    "    # Free-up CUDA\n",
    "    del std_dataset\n",
    "\n",
    "    # Compute predictions and corresponding uncertainties\n",
    "    shot_predictions, shot_uncertainties = make_predictions(reference_dataset, std_dataset_w_temp, model, standardized_parameters)\n",
    "\n",
    "    # Free-up CUDA\n",
    "    del std_dataset_w_temp\n",
    "\n",
    "    # Computing the coefficients and uncertainties from fitting\n",
    "    coefficients = compute_coefficients(temperatures, shot_predictions, shot_uncertainties, s=0.5)\n",
    "    \n",
    "    # Compute Fv\n",
    "    Fv_pred = compute_Fv(temperatures, coefficients)[0]\n",
    "    return  Fv_pred, shot_uncertainties\n",
    "\n",
    "def pred_DFT_Fv(path_to_PHONON, temperatures):\n",
    "    # Loading number of atoms\n",
    "    _, _, concentration, _ = MPL.information_from_VASPfile(path_to_PHONON, file='POSCAR')\n",
    "    n_atoms = np.sum(concentration)\n",
    "    \n",
    "    # Reading supercell information\n",
    "    dim_info = MPL.read_phonopyconf(path_to_PHONON)\n",
    "    \n",
    "    # Write mesh.conf file (needed for phonopy)\n",
    "    MPL.write_meshconf(path_to_PHONON, material, dim_info, Ti, Tf, dT)\n",
    "    \n",
    "    # Getting thermal properties with phonopy (ignoring output)\n",
    "    previous_dir = os.getcwd()\n",
    "    os.chdir(path_to_PHONON)\n",
    "    os.system('phonopy -t mesh.conf > /dev/null')\n",
    "    os.chdir(previous_dir)\n",
    "    \n",
    "    # Read generated thermal properties (kJ/mol)\n",
    "    _, Fv_PHONON = MPL.read_thermalpropertyyaml(len(temperatures), path_to_PHONON, thermalproperty='free_energy')\n",
    "    \n",
    "    # Pass kJ / molmp-1009220 to meV / atom\n",
    "    conversion_factor = 1.6 * 6.022 * 0.01 * n_atoms\n",
    "    Fv_PHONON        /= conversion_factor\n",
    "    return Fv_PHONON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aff6226-ad07-4f0e-a09b-2575f28ca14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(path_to_MP):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m Fv_ML_MP_0, uncert_ML_MP_0 = \u001b[43mpred_ML_Fv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_MP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                                        \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstandardized_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m Fv_ML_phonons_0, uncert_ML_phonons_0 = pred_ML_Fv(path_to_phonons,\n\u001b[32m     25\u001b[39m                                                   temperatures, standardized_parameters, model_folder, device)\n\u001b[32m     26\u001b[39m Fv_DFT_0 = pred_DFT_Fv(path_to_phonons,\n\u001b[32m     27\u001b[39m                        temperatures)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mpred_ML_Fv\u001b[39m\u001b[34m(path_to_POSCAR, temperatures, standardized_parameters, model_folder, device)\u001b[39m\n\u001b[32m      8\u001b[39m std_dataset = standardize_dataset_from_keys(dataset, standardized_parameters)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load Graph Neural Network model (making room for temperature as node attribute) to device\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Dropout for initializing the model, not used at all while predicting\u001b[39;00m\n\u001b[32m     12\u001b[39m model = \u001b[43mGCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_node_features\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m             \u001b[49m\u001b[43mpdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Free-up CUDA\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cibran/Work/UCL/ml-phasetransitions/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cibran/Work/UCL/ml-phasetransitions/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cibran/Work/UCL/ml-phasetransitions/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cibran/Work/UCL/ml-phasetransitions/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cibran/Work/UCL/ml-phasetransitions/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "uncert_DFT_0 = np.ones(len(temperatures)) * 4\n",
    "uncert_DFT_1 = np.ones(len(temperatures)) * 4\n",
    "\n",
    "uplims = [False] * len(temperatures)\n",
    "lolims = [False] * len(temperatures)\n",
    "\n",
    "for material in materials.keys():\n",
    "    print(material)\n",
    "\n",
    "    polymorphs = materials[material]\n",
    "    path_to_material = f'/home/claudio/Desktop/validation/{material}'\n",
    "\n",
    "    # IDX = 0\n",
    "    idx = 0\n",
    "    path_to_polymorph = f'{path_to_material}/{polymorphs[idx]}'\n",
    "    path_to_MP = f'{path_to_polymorph}/relaxation'\n",
    "    path_to_phonons = f'{path_to_polymorph}/phonons'\n",
    "\n",
    "    if not os.path.exists(path_to_MP):\n",
    "        continue\n",
    "    \n",
    "    Fv_ML_MP_0, uncert_ML_MP_0 = pred_ML_Fv(path_to_MP,\n",
    "                                            temperatures, standardized_parameters, model_folder, device)\n",
    "    Fv_ML_phonons_0, uncert_ML_phonons_0 = pred_ML_Fv(path_to_phonons,\n",
    "                                                      temperatures, standardized_parameters, model_folder, device)\n",
    "    Fv_DFT_0 = pred_DFT_Fv(path_to_phonons,\n",
    "                           temperatures)\n",
    "    \n",
    "    epa_0 = float(np.loadtxt(f'{path_to_MP}/EPA')) * 1e3  # From eV/atom to meV/atom\n",
    "\n",
    "    \n",
    "    # IDX = 1\n",
    "    idx = 1\n",
    "    path_to_polymorph = f'{path_to_material}/{polymorphs[idx]}'\n",
    "    path_to_MP = f'{path_to_polymorph}/relaxation'\n",
    "    path_to_phonons = f'{path_to_polymorph}/phonons'\n",
    "\n",
    "    if not os.path.exists(path_to_MP):\n",
    "        continue\n",
    "    \n",
    "    Fv_ML_MP_1, uncert_ML_MP_1 = pred_ML_Fv(path_to_MP,\n",
    "                                            temperatures, standardized_parameters, model_folder, device)\n",
    "    Fv_ML_phonons_1, uncert_ML_phonons_1 = pred_ML_Fv(path_to_phonons,\n",
    "                                                      temperatures, standardized_parameters, model_folder, device)\n",
    "    Fv_DFT_1 = pred_DFT_Fv(path_to_phonons,\n",
    "                           temperatures)\n",
    "    \n",
    "    epa_1 = float(np.loadtxt(f'{path_to_MP}/EPA')) * 1e3  # From eV/atom to meV/atom\n",
    "\n",
    "    \n",
    "    # Diff\n",
    "    F_ML_MP_diff = (epa_1+Fv_ML_MP_1) - (epa_0+Fv_ML_MP_0)\n",
    "    uncert_ML_MP_diff = np.sqrt(uncert_ML_MP_0**2 + uncert_ML_MP_1**2)\n",
    "\n",
    "    F_ML_phonons_diff = (epa_1+Fv_ML_phonons_1) - (epa_0+Fv_ML_phonons_0)\n",
    "    uncert_ML_phonons_diff = np.sqrt(uncert_ML_phonons_0**2 + uncert_ML_phonons_1**2)\n",
    "\n",
    "    F_DFT_diff = (epa_1+Fv_DFT_1) - (epa_0+Fv_DFT_0)\n",
    "    uncert_DFT_diff = np.sqrt(uncert_DFT_0**2 + uncert_DFT_1**2)\n",
    "    \n",
    "    # Plotting\n",
    "    label = f'{polymorphs[1]}-{polymorphs[0]}'\n",
    "    plt.errorbar(temperatures, F_ML_MP_diff,      color=colors[0], yerr=uncert_ML_MP_diff,      label='ML original', \n",
    "                 fmt=':o', uplims=uplims, lolims=lolims)\n",
    "    plt.errorbar(temperatures, F_ML_phonons_diff, color=colors[1], yerr=uncert_ML_phonons_diff, label='ML relaxed',\n",
    "                 fmt=':o', uplims=uplims, lolims=lolims)\n",
    "    plt.errorbar(temperatures, F_DFT_diff,        color=colors[2], yerr=uncert_DFT_diff,        label='DFT',\n",
    "                 fmt=':o', uplims=uplims, lolims=lolims)\n",
    "        \n",
    "    plt.xlabel(r'$T$ (K)')\n",
    "    plt.ylabel(r'$\\Delta F$ (meV/atom)')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(f'{path_to_material}/{material}-diff.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
