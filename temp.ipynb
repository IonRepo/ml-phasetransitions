{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0800a3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T11:25:15.471260Z",
     "start_time": "2025-03-11T11:25:14.011367Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from libraries.model   import Helmholtz_free_energy_function, make_predictions, GCNN, compute_coefficients\n",
    "from libraries.dataset import load_atomic_masses, include_temperatures, create_predictions_dataset, standardize_dataset_from_keys\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25641cce",
   "metadata": {},
   "source": [
    "# Compute phase transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e84d65-495c-4b79-9a1b-5d41bb0623e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = '/home/claudio/cibran/Work/UPC/MP/models/Fv-accurate-fulldata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d13e531-1bde-4002-b084-ccb0ceee30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_name                 = f'{target_folder}/labels.pt'\n",
    "dataset_name                = f'{target_folder}/dataset.pt'\n",
    "dataset_name_std            = f'{target_folder}/standardized_dataset.pt'\n",
    "labels_name_std             = f'{target_folder}/standardized_labels.pt'\n",
    "dataset_parameters_name_std = f'{target_folder}/standardized_parameters.json'  # Parameters for rescaling the predictions\n",
    "\n",
    "# Load the standardized dataset, with corresponding labels and parameters\n",
    "dataset = torch.load(dataset_name_std, weights_only=False)\n",
    "labels  = torch.load(labels_name_std,  weights_only=False)\n",
    "for i, data in enumerate(dataset):\n",
    "    data.label = labels[i]\n",
    "\n",
    "# Convert NumPy arrays back to PyTorch tensors\n",
    "# Load the data from the JSON file\n",
    "with open(dataset_parameters_name_std, 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert torch tensors to numpy arrays\n",
    "dataset_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        dataset_parameters[key] = torch.tensor(value)\n",
    "    except:\n",
    "        dataset_parameters[key] = value\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = dataset_parameters['target_std'] / dataset_parameters['scale']\n",
    "target_factor = target_factor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671255b0-4f4d-45ae-8b1b-fea73a03dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../UPC')\n",
    "from GenerativeModels.libraries.dataset import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5e7145-6ded-48c0-bf17-c67b78a6d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   graphs: 58729\n",
      "Number of validation graphs: 7348\n",
      "Number of testing    graphs: 7337\n"
     ]
    }
   ],
   "source": [
    "# Check if data has been already split, else do it randomly\n",
    "path_to_train_labels = f'{target_folder}/train_labels.txt'\n",
    "path_to_val_labels   = f'{target_folder}/validation_labels.txt'\n",
    "path_to_test_labels  = f'{target_folder}/test_labels.txt'\n",
    "\n",
    "# Copy labels\n",
    "material_labels = labels.copy()\n",
    "\n",
    "# Read labels splitting (which are strings)\n",
    "train_labels = np.genfromtxt(path_to_train_labels, dtype='str').tolist()\n",
    "val_labels   = np.genfromtxt(path_to_val_labels,   dtype='str').tolist()\n",
    "test_labels  = np.genfromtxt(path_to_test_labels,  dtype='str').tolist()\n",
    "\n",
    "# Use the computed indexes to generate train and test sets\n",
    "# We iteratively check where labels equals a unique train/test labels and append the index to a list\n",
    "train_dataset = get_datasets(train_labels, material_labels, dataset)\n",
    "val_dataset   = get_datasets(val_labels,   material_labels, dataset)\n",
    "test_dataset  = get_datasets(test_labels,  material_labels, dataset)\n",
    "\n",
    "#del dataset  # Free up CUDA memory\n",
    "\n",
    "print(f'Number of training   graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of testing    graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b859b10d-8ecc-4516-b894-b8a56678b0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNN(\n",
       "  (conv1): GraphConv(5, 512)\n",
       "  (conv2): GraphConv(512, 512)\n",
       "  (linconv1): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (linconv2): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (lin): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the JSON file\n",
    "with open(f'{target_folder}/standardized_parameters.json', 'r') as json_file:\n",
    "    numpy_dict = json.load(json_file)\n",
    "\n",
    "# Convert NumPy arrays back to PyTorch tensors\n",
    "standardized_parameters = {}\n",
    "for key, value in numpy_dict.items():\n",
    "    try:\n",
    "        standardized_parameters[key] = torch.tensor(value)\n",
    "    except:\n",
    "        standardized_parameters[key] = value\n",
    "\n",
    "# Load Graph Neural Network model (making room for temperature as node attribute) to device\n",
    "# Dropout for initializing the model, not used at all while predicting\n",
    "model = GCNN(features_channels=train_dataset[0].num_node_features,\n",
    "             pdropout=0).to(device)\n",
    "\n",
    "# Load and evaluate Graph Neural Network model\n",
    "model.load_state_dict(torch.load(f'{target_folder}/model.pt', map_location=torch.device(device)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab57f619-c645-4f10-9dcd-f3af8257ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0].x[0][-1] < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac2f51-2199-4cd3-b7f7-9bf6a7edee0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1fe693-ee46-4a44-b2bd-ab4281c766de",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = standardized_parameters['target_mean']\n",
    "scale       = standardized_parameters['scale']\n",
    "target_std  = standardized_parameters['target_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56b783bf-25ec-4af0-88d2-a378eb3f5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_dataset = test_dataset\n",
    "reference_dataset = []\n",
    "for data in test_dataset:\n",
    "    if data.x[0][-1] < 10:\n",
    "        reference_dataset.append(data)\n",
    "\n",
    "#prediction_dataset = train_dataset\n",
    "prediction_dataset = []\n",
    "for data in train_dataset:\n",
    "    if data.x[0][-1] < 10:\n",
    "        prediction_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e646dc-5551-491b-92b1-becc8847cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions and corresponding uncertainties\n",
    "predictions, uncertainties = make_predictions(reference_dataset, prediction_dataset, model, standardized_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe3906-5670-4a14-b56f-70ddd6c3d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = []\n",
    "for i, data in enumerate(prediction_dataset):\n",
    "    ground_truth = data.y.cpu().numpy()[0] * target_std / scale + target_mean\n",
    "    ground_truths.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683e4d6-ec15-46f0-9503-af99bd7b5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ground_truths, predictions, 'og', label='Validation set')  # Predictions\n",
    "for x, y, err in zip(ground_truths, predictions, uncertainties):\n",
    "    plt.plot([x, x], [y - err, y + err], 'g--')  # Vertical line\n",
    "\n",
    "_min_ = np.min([np.min(ground_truths), np.min(predictions-uncertainties)])\n",
    "_max_ = np.max([np.max(ground_truths), np.max(predictions+uncertainties)])\n",
    "plt.plot([_min_, _max_], [_min_, _max_], '-r')  # Identity line\n",
    "plt.xlabel('Computed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('UQ-prediction-comparison.pdf', dpi=50, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22c79e-66ec-4f0f-8ee7-53fff0e1422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_real = np.abs(predictions - ground_truths)\n",
    "diff_predictions = uncertainties\n",
    "\n",
    "# semi-transparent points\n",
    "plt.plot(diff_real, diff_predictions, 'o', markeredgecolor='none', \n",
    "         alpha=0.1, label='Validation set')\n",
    "\n",
    "# identity line\n",
    "_min_ = np.min([np.min(diff_real), np.min(diff_predictions)])\n",
    "_max_ = np.max([np.max(diff_real), np.max(diff_predictions)])\n",
    "plt.plot([_min_, _max_], [_min_, _max_], '-r', label='Ideal calibration')\n",
    "\n",
    "plt.xlabel('Computed uncertainty')\n",
    "plt.ylabel('Predicted uncertainty')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('UQ-diff-comparison.pdf', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
